import os
import requests
from datetime import datetime, timedelta, timezone
import time
from typing import List, Dict, Optional
import logging
import importlib

# Optional Polygon SDK (dynamic import to avoid hard dependency)
PolygonRESTClient = None
PolygonTickerNews = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NewsCollector:
    def __init__(self, polygon_api_key: Optional[str] = None):
        # Polygon Stocks News API
        self.polygon_api_key = polygon_api_key or os.getenv("POLYGON_API_KEY")
        self.polygon_base_url = "https://api.polygon.io/v2/reference/news"
        # Initialize SDK client if available
        self._polygon_client = None
        if self.polygon_api_key:
            try:
                polygon_mod = importlib.import_module('polygon')
                rest_client_cls = getattr(polygon_mod, 'RESTClient', None)
                if rest_client_cls:
                    global PolygonRESTClient
                    PolygonRESTClient = rest_client_cls
                    # Try to import TickerNews model
                    try:
                        models_mod = importlib.import_module('polygon.rest.models')
                        global PolygonTickerNews
                        PolygonTickerNews = getattr(models_mod, 'TickerNews', None)
                    except Exception:
                        PolygonTickerNews = None

                    self._polygon_client = PolygonRESTClient(self.polygon_api_key)
                    logger.info("Polygon SDK í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©")
            except Exception as e:
                logger.info(f"Polygon SDK ë¯¸ì‚¬ìš©(ë™ì  ì„í¬íŠ¸ ì‹¤íŒ¨ ë˜ëŠ” ë¯¸ì„¤ì¹˜): {e}")

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.last_api_call = 0
        self.min_call_interval = 1.0  # 1ì´ˆ ê°„ê²©

    @staticmethod
    def _to_rfc3339_z(dt: datetime) -> str:
        """Format datetime to RFC3339 (ISO8601) in UTC ending with 'Z'."""
        if dt.tzinfo is None:
            # Naive datetime as local time; convert to UTC
            try:
                offset_sec = -time.timezone if (time.daylight == 0) else -time.altzone
                local_tz = timezone(timedelta(seconds=offset_sec))
                dt = dt.replace(tzinfo=local_tz)
            except Exception:
                dt = dt.replace(tzinfo=timezone.utc)
        dt_utc = dt.astimezone(timezone.utc)
        return dt_utc.strftime('%Y-%m-%dT%H:%M:%SZ')

    def _wait_for_rate_limit(self):
        """API í˜¸ì¶œ ê°„ê²© ì œí•œ"""
        current_time = time.time()
        time_since_last_call = current_time - self.last_api_call

        if time_since_last_call < self.min_call_interval:
            wait_time = self.min_call_interval - time_since_last_call
            logger.info(f"â³ API í˜¸ì¶œ ì œí•œìœ¼ë¡œ {wait_time:.1f}ì´ˆ ëŒ€ê¸°")
            time.sleep(wait_time)

        self.last_api_call = time.time()

    def search_polygon(self, ticker: str, from_date: datetime, to_date: datetime, max_articles: int = 50) -> List[Dict]:
        """Polygon Stocks News APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰.

        ìš°ì„  Polygon SDKê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ SDKë¥¼ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ HTTP ìš”ì²­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
        """
        self._wait_for_rate_limit()

        # Normalize and guard date range
        if from_date and to_date and from_date > to_date:
            from_date, to_date = to_date, from_date

        # Build RFC3339 strings in UTC
        gte = self._to_rfc3339_z(from_date) if from_date else None
        lte = self._to_rfc3339_z(to_date) if to_date else None

        if not self.polygon_api_key:
            logger.error("POLYGON_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ìƒì„±ì ì¸ìë¡œ ì œê³µí•˜ì„¸ìš”.")
            return []

        # Prefer SDK if available
        if self._polygon_client is not None:
            try:
                logger.info(
                    f"ğŸ” Polygon SDK ê²€ìƒ‰: '{ticker}' (ê¸°ê°„: {gte or '-'} ~ {lte or '-'})"
                )
                items: List[Dict] = []

                # polygon-api-clientëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ë°˜í™˜
                gen = self._polygon_client.list_ticker_news(
                    ticker=ticker,
                    published_utc_gte=gte,
                    published_utc_lte=lte,
                    order="desc",
                    sort="published_utc",
                    limit=min(max_articles, 1000),
                )
                count = 0
                for n in gen:
                    count += 1
                    # ëª¨ë¸ ë˜ëŠ” dict í˜•íƒœ ëª¨ë‘ ì§€ì›
                    if PolygonTickerNews is not None and isinstance(n, PolygonTickerNews):
                        published_date = None
                        try:
                            # SDK ëª¨ë¸ì˜ published_utcëŠ” datetimeì¼ ìˆ˜ ìˆìŒ
                            published_date = (
                                n.published_utc if isinstance(n.published_utc, datetime)
                                else datetime.fromisoformat(str(n.published_utc).replace('Z', '+00:00'))
                            )
                        except Exception:
                            published_date = None

                        news_item = {
                            'title': getattr(n, 'title', '') or '',
                            'description': getattr(n, 'description', '') or '',
                            'content': None,
                            'link': getattr(n, 'article_url', '') or '',
                            'published_date': published_date,
                            'source': (getattr(getattr(n, 'publisher', None), 'name', None) or 'Unknown'),
                            'image': getattr(n, 'image_url', '') or '',
                        }
                    else:
                        # dict-like
                        pub_str = (n.get('published_utc') if isinstance(n, dict) else None)
                        published_date = None
                        if pub_str:
                            try:
                                published_date = datetime.fromisoformat(str(pub_str).replace('Z', '+00:00'))
                            except Exception:
                                published_date = None

                        publisher = (n.get('publisher') if isinstance(n, dict) else {}) or {}
                        news_item = {
                            'title': (n.get('title') if isinstance(n, dict) else '') or '',
                            'description': (n.get('description') if isinstance(n, dict) else '') or '',
                            'content': None,
                            'link': (n.get('article_url') if isinstance(n, dict) else '') or '',
                            'published_date': published_date,
                            'source': publisher.get('name', 'Unknown'),
                            'image': (n.get('image_url') if isinstance(n, dict) else '') or '',
                        }

                    if len(items) < max_articles:
                        items.append(news_item)
                    else:
                        break

                logger.info(f"ğŸ“° Polygon SDK: {len(items)}ê°œ ë‰´ìŠ¤ ë°œê²¬ (ì›ì‹œ {count}ê°œ)")
                return items
            except Exception as e:
                logger.warning(f"Polygon SDK í˜¸ì¶œ ì˜¤ë¥˜, HTTPë¡œ ëŒ€ì²´: {e}")
                # Fall through to HTTP

        # HTTP fallback
        params = {
            'ticker': ticker,
            'order': 'desc',
            'sort': 'published_utc',
            'limit': min(max_articles, 1000),
            'apiKey': self.polygon_api_key,
        }
        if gte:
            params['published_utc.gte'] = gte
        if lte:
            params['published_utc.lte'] = lte

        try:
            logger.info(f"ğŸ” Polygon HTTP ê²€ìƒ‰: '{ticker}' (ê¸°ê°„: {gte or '-'} ~ {lte or '-'})")
            response = requests.get(self.polygon_base_url, params=params, headers=self.headers, timeout=30)
            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                logger.info(f"ğŸ“° Polygon HTTP: {len(results)}ê°œ ë‰´ìŠ¤ ë°œê²¬")

                news_items: List[Dict] = []
                for i, article in enumerate(results):
                    try:
                        published_date = None
                        pub_str = article.get('published_utc')
                        if pub_str:
                            published_date = datetime.fromisoformat(str(pub_str).replace('Z', '+00:00'))

                        publisher = article.get('publisher') or {}
                        news_item = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'content': None,
                            'link': article.get('article_url', ''),
                            'published_date': published_date,
                            'source': publisher.get('name', 'Unknown'),
                            'image': article.get('image_url', ''),
                        }
                        news_items.append(news_item)

                        if i < 3:
                            logger.info(f"  ğŸ“„ ë‰´ìŠ¤ {i+1}: {news_item['title'][:50]}...")
                            logger.info(f"      ë°œí–‰ì¼: {published_date}")
                            logger.info(f"      ì¶œì²˜: {news_item['source']}")
                    except Exception as e:
                        logger.warning(f"ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logger.warning(f"ì›ë³¸ ë°ì´í„°: {article}")
                        continue

                logger.info(f"âœ… ì´ {len(news_items)}ê°œ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì™„ë£Œ")
                return news_items
            elif response.status_code == 403:
                logger.error("âŒ Polygon API ì¸ì¦ ì‹¤íŒ¨ - API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                return []
            elif response.status_code == 429:
                logger.warning("âš ï¸ Polygon API í˜¸ì¶œ ì œí•œ ì´ˆê³¼ - ì ì‹œ ëŒ€ê¸°")
                time.sleep(60)
                return []
            else:
                logger.error(f"âŒ Polygon API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            logger.error(f"Polygon HTTP í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def collect_company_news(self, company_name: str, ticker: str, target_date: datetime) -> List[Dict]:
        """íŠ¹ì • íšŒì‚¬ì˜ íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘ (Polygon API ì‚¬ìš©)"""
        all_news: List[Dict] = []

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ë‹¹ì¼ ì „ì²´)
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        logger.info(f"ğŸ“° {ticker} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"ğŸ“… ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} (ë‹¹ì¼ë§Œ)")

        # Polygonì€ í‹°ì»¤ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
        keywords = [ticker]

        # ê° í‚¤ì›Œë“œ(í‹°ì»¤)ë¡œ ê²€ìƒ‰
        for i, keyword in enumerate(keywords, 1):
            try:
                logger.info(f"ğŸ” í‚¤ì›Œë“œ {i}: '{keyword}' ê²€ìƒ‰ ì¤‘...")

                # Polygon APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì •í™•í•œ ì¼ì ë²”ìœ„ ì „ë‹¬)
                news_items = self.search_polygon(
                    ticker=keyword,
                    from_date=start_date,
                    to_date=end_date,
                    max_articles=100,
                )

                if news_items:
                    # ëŒ€ìƒ ë‚ ì§œ í•„í„°ë§
                    filtered_news = self.filter_news_by_date(news_items, target_date)
                    all_news.extend(filtered_news)

                    logger.info(f"  âœ… '{keyword}': {len(filtered_news)}ê°œ ê´€ë ¨ ë‰´ìŠ¤")
                else:
                    logger.info(f"  âŒ '{keyword}': ë‰´ìŠ¤ ì—†ìŒ")

                # API í˜¸ì¶œ ì œí•œ ëŒ€ê¸°
                time.sleep(1)

            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue

        # ì¤‘ë³µ ì œê±° (ì œëª©ê³¼ URL ê¸°ì¤€)
        seen_items = set()
        unique_news: List[Dict] = []

        for news in all_news:
            # ì œëª©ê³¼ URLì„ ê²°í•©í•œ í‚¤ ìƒì„±
            key = f"{news['title'].strip().lower()}_{news['link']}"

            if key not in seen_items and len(news['title'].strip()) > 10:
                seen_items.add(key)
                unique_news.append(news)

        logger.info(f"ğŸ¯ {ticker} ({target_date.strftime('%Y-%m-%d')}): ì´ {len(unique_news)}ê°œ ìœ ë‹ˆí¬ ë‰´ìŠ¤ ìˆ˜ì§‘")

        # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìš”ì•½ ë¡œê¹…
        if unique_news:
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì¶œì²˜ ë¶„í¬:")
            sources: Dict[str, int] = {}
            for news in unique_news:
                source = news['source']
                sources[source] = sources.get(source, 0) + 1

            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  - {source}: {count}ê°œ")

        return unique_news

    def filter_news_by_date(self, news_items: List[Dict], target_date: datetime) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œ(ë‹¹ì¼) ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        # ì •í™•íˆ ë‹¹ì¼ë§Œ í¬í•¨
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)

        logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§ (ë‹¹ì¼ë§Œ): {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"ğŸ“¥ í•„í„°ë§ ì „ ë‰´ìŠ¤ ìˆ˜: {len(news_items)}")

        filtered_news: List[Dict] = []
        for i, news in enumerate(news_items):
            # ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš° í¬í•¨ (ìµœì‹  ë‰´ìŠ¤ë¡œ ê°„ì£¼)
            if not news['published_date']:
                logger.info(f"  âš ï¸  ë‰´ìŠ¤ {i+1}: ë‚ ì§œ ì—†ìŒ -> í¬í•¨")
                filtered_news.append(news)
                continue

            # UTC ì‹œê°„ì„ ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            pub_date = news['published_date']
            if pub_date.tzinfo:
                pub_date = pub_date.replace(tzinfo=None)

            # ë‚ ì§œ ë¹„êµ ë¡œê¹…
            is_in_range = start_date <= pub_date <= end_date

            # ì²˜ìŒ 5ê°œë§Œ ìƒì„¸ ë¡œê¹…
            if i < 5:
                logger.info(f"  ğŸ“° ë‰´ìŠ¤ {i+1}: {pub_date.strftime('%Y-%m-%d')} -> {'í¬í•¨' if is_in_range else 'ì œì™¸'}")

            if is_in_range:
                filtered_news.append(news)

        logger.info(f"ğŸ“¤ í•„í„°ë§ í›„ ë‰´ìŠ¤ ìˆ˜: {len(filtered_news)}")

        # ê²°ê³¼ê°€ ì—†ì–´ë„ ë‹¹ì¼ë§Œ ìœ ì§€ (ì™„í™”í•˜ì§€ ì•ŠìŒ)
        return filtered_news

    def get_news_text(self, news_items: List[Dict]) -> str:
        """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
        if not news_items:
            return ""

        texts = []
        for news in news_items:
            # ì œëª©, ì„¤ëª…, ë‚´ìš©ì„ ëª¨ë‘ ê²°í•©
            text_parts = []

            if news.get('title'):
                text_parts.append(news['title'])

            if news.get('description'):
                text_parts.append(news['description'])

            if news.get('content'):
                # contentê°€ ìˆìœ¼ë©´ description ëŒ€ì‹  ì‚¬ìš©
                text_parts.append(news['content'])

            # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
            text = '. '.join(text_parts)
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = ' '.join(text.split())

            if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                texts.append(text)

        combined_text = ' | '.join(texts)

        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        if len(combined_text) > 8000:
            combined_text = combined_text[:8000] + "..."

        return combined_text


if __name__ == "__main__":
    collector = NewsCollector(polygon_api_key="q96aIisakzHv_c7jRaoginkjRj8zGWu3")
    test_date = datetime(2024, 7, 15)
    print("=== Polygon Stocks News API í…ŒìŠ¤íŠ¸ ===")
    news = collector.collect_company_news("Apple Inc.", "AAPL", test_date)
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
    if news:
        print("\nì²« ë²ˆì§¸ ë‰´ìŠ¤:")
        print(f"ì œëª©: {news[0]['title']}")
        print(f"ì„¤ëª…: {news[0]['description'][:200]}...")
        print(f"ì¶œì²˜: {news[0]['source']}")
        print(f"ë‚ ì§œ: {news[0]['published_date']}")
