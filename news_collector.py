import requests
from datetime import datetime, timedelta, timezone
import time
from typing import List, Dict
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self, gnews_api_key: str = "44cc3fd97fb6417d55aeb8d962f7a831"):
        self.gnews_api_key = gnews_api_key
        self.gnews_base_url = "https://gnews.io/api/v4/search"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.last_api_call = 0
        self.min_call_interval = 1.0  # 1ì´ˆ ê°„ê²©
        
    @staticmethod
    def _to_gnews_iso(dt: datetime) -> str:
        """Format datetime to GNews ISO 8601 in UTC ending with 'Z'.

        - Accepts naive or timezone-aware datetimes.
        - Naive datetimes are assumed to be in local time and converted to UTC.
        - Milliseconds are optional in the API; we include .000Z for consistency with docs.
        """
        if dt.tzinfo is None:
            # Treat naive datetime as local time, convert to UTC
            # If local tz info isn't available, assume it's already UTC
            try:
                # Python 3.9+: no built-in local tz; best-effort: attach local offset via time module
                offset_sec = -time.timezone if (time.daylight == 0) else -time.altzone
                local_tz = timezone(timedelta(seconds=offset_sec))
                dt = dt.replace(tzinfo=local_tz)
            except Exception:
                dt = dt.replace(tzinfo=timezone.utc)
        dt_utc = dt.astimezone(timezone.utc)
        return dt_utc.strftime('%Y-%m-%dT%H:%M:%S.000Z')

    def _wait_for_rate_limit(self):
        """API í˜¸ì¶œ ê°„ê²© ì œí•œ"""
        current_time = time.time()
        time_since_last_call = current_time - self.last_api_call
        
        if time_since_last_call < self.min_call_interval:
            wait_time = self.min_call_interval - time_since_last_call
            logger.info(f"â³ API í˜¸ì¶œ ì œí•œìœ¼ë¡œ {wait_time:.1f}ì´ˆ ëŒ€ê¸°")
            time.sleep(wait_time)
        
        self.last_api_call = time.time()
    
    def search_gnews(self, query: str, from_date: datetime, to_date: datetime, max_articles: int = 10) -> List[Dict]:
        """GNews APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰

        Note: Uses 'from' and 'to' parameters per GNews docs (ISO 8601 UTC with 'Z').
        """
        self._wait_for_rate_limit()
        
        # Normalize and guard date range
        if from_date and to_date and from_date > to_date:
            from_date, to_date = to_date, from_date

        # Build ISO8601 strings in UTC
        from_str = self._to_gnews_iso(from_date) if from_date else None
        # Inclusive end of day if time not set precisely
        to_str = self._to_gnews_iso(to_date) if to_date else None

        # API parameters per docs
        params = {
            'q': query,
            # Docs use 'apikey'; legacy 'token' may also work. Prefer 'apikey'.
            'apikey': self.gnews_api_key,
            'lang': 'en',
            'country': 'us',
            'max': min(max_articles, 100),  # API ìµœëŒ€ 100ê°œ ì œí•œ
            'sortby': 'publishedAt',  # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
            'in': 'title,description,content'
        }

        if from_str:
            params['from'] = from_str
        if to_str:
            params['to'] = to_str
        
        try:
            if from_str or to_str:
                logger.info(f"ğŸ” GNews API ê²€ìƒ‰: '{query}' (ê¸°ê°„: {from_str or '-'} ~ {to_str or '-'})")
            else:
                logger.info(f"ğŸ” GNews API ê²€ìƒ‰: '{query}' (ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰)")
            
            response = requests.get(self.gnews_base_url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                logger.info(f"ğŸ“° GNews API: {len(articles)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                # ë‰´ìŠ¤ ë°ì´í„° ë³€í™˜
                news_items = []
                for i, article in enumerate(articles):
                    try:
                        # ë‚ ì§œ íŒŒì‹±
                        published_date = None
                        if 'publishedAt' in article:
                            published_date = datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00'))
                        
                        news_item = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'content': article.get('content', ''),
                            'link': article.get('url', ''),
                            'published_date': published_date,
                            'source': article.get('source', {}).get('name', 'Unknown'),
                            'image': article.get('image', '')
                        }
                        news_items.append(news_item)
                        
                        # ì²˜ìŒ 3ê°œ ë‰´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ ë¡œê¹…
                        if i < 3:
                            logger.info(f"  ğŸ“„ ë‰´ìŠ¤ {i+1}: {news_item['title'][:50]}...")
                            logger.info(f"      ë°œí–‰ì¼: {published_date}")
                            logger.info(f"      ì¶œì²˜: {news_item['source']}")
                        
                    except Exception as e:
                        logger.warning(f"ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logger.warning(f"ì›ë³¸ ë°ì´í„°: {article}")
                        continue
                
                logger.info(f"âœ… ì´ {len(news_items)}ê°œ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì™„ë£Œ")
                return news_items
                
            elif response.status_code == 403:
                logger.error("âŒ GNews API ì¸ì¦ ì‹¤íŒ¨ - API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                return []
            elif response.status_code == 429:
                logger.warning("âš ï¸ GNews API í˜¸ì¶œ ì œí•œ ì´ˆê³¼ - ì ì‹œ ëŒ€ê¸°")
                time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
                return []
            else:
                logger.error(f"âŒ GNews API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"GNews API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def get_company_keywords(self, company_name: str, ticker: str) -> List[str]:
        """íšŒì‚¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        keywords = [ticker]
        
        # ê¸°ë³¸ íšŒì‚¬ëª…
        keywords.append(f'"{company_name}"')
        
        return keywords
    
    def collect_company_news(self, company_name: str, ticker: str, target_date: datetime) -> List[Dict]:
        """íŠ¹ì • íšŒì‚¬ì˜ íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘ (GNews API ì‚¬ìš©)"""
        all_news = []
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ë‹¹ì¼ ì „ì²´)
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
            
        logger.info(f"ğŸ“° {ticker} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"ğŸ“… ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} (ë‹¹ì¼ë§Œ)")
        
        # íšŒì‚¬ë³„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keywords = self.get_company_keywords(company_name, ticker)
        
        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for i, keyword in enumerate(keywords, 1):  # ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš© (í‹°ì»¤ + íšŒì‚¬ëª…)
            try:
                logger.info(f"ğŸ” í‚¤ì›Œë“œ {i}: '{keyword}' ê²€ìƒ‰ ì¤‘...")
                
                # GNews APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì •í™•í•œ ì¼ì ë²”ìœ„ ì „ë‹¬)
                news_items = self.search_gnews(keyword, start_date, end_date, max_articles=20)
                
                if news_items:
                    # ëŒ€ìƒ ë‚ ì§œ í•„í„°ë§
                    filtered_news = self.filter_news_by_date(news_items, target_date)
                    all_news.extend(filtered_news)
                    
                    logger.info(f"  âœ… '{keyword}': {len(filtered_news)}ê°œ ê´€ë ¨ ë‰´ìŠ¤")
                else:
                    logger.info(f"  âŒ '{keyword}': ë‰´ìŠ¤ ì—†ìŒ")
                
                # API í˜¸ì¶œ ì œí•œ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (ì œëª©ê³¼ URL ê¸°ì¤€)
        seen_items = set()
        unique_news = []
        
        for news in all_news:
            # ì œëª©ê³¼ URLì„ ê²°í•©í•œ í‚¤ ìƒì„±
            key = f"{news['title'].strip().lower()}_{news['link']}"
            
            if key not in seen_items and len(news['title'].strip()) > 10:
                seen_items.add(key)
                unique_news.append(news)
        
        logger.info(f"ğŸ¯ {ticker} ({target_date.strftime('%Y-%m-%d')}): ì´ {len(unique_news)}ê°œ ìœ ë‹ˆí¬ ë‰´ìŠ¤ ìˆ˜ì§‘")
        
        # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìš”ì•½ ë¡œê¹…
        if unique_news:
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì¶œì²˜ ë¶„í¬:")
            sources = {}
            for news in unique_news:
                source = news['source']
                sources[source] = sources.get(source, 0) + 1
            
            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  - {source}: {count}ê°œ")
        
        return unique_news
    
    def filter_news_by_date(self, news_items: List[Dict], target_date: datetime) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œ(ë‹¹ì¼) ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        # ì •í™•íˆ ë‹¹ì¼ë§Œ í¬í•¨
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§ (ë‹¹ì¼ë§Œ): {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"ğŸ“¥ í•„í„°ë§ ì „ ë‰´ìŠ¤ ìˆ˜: {len(news_items)}")
        
        filtered_news = []
        for i, news in enumerate(news_items):
            # ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš° í¬í•¨ (ìµœì‹  ë‰´ìŠ¤ë¡œ ê°„ì£¼)
            if not news['published_date']:
                logger.info(f"  âš ï¸  ë‰´ìŠ¤ {i+1}: ë‚ ì§œ ì—†ìŒ -> í¬í•¨")
                filtered_news.append(news)
                continue
            
            # UTC ì‹œê°„ì„ ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            pub_date = news['published_date']
            if pub_date.tzinfo:
                pub_date = pub_date.replace(tzinfo=None)
            
            # ë‚ ì§œ ë¹„êµ ë¡œê¹…
            is_in_range = start_date <= pub_date <= end_date
            
            # ì²˜ìŒ 5ê°œë§Œ ìƒì„¸ ë¡œê¹…
            if i < 5:
                logger.info(f"  ğŸ“° ë‰´ìŠ¤ {i+1}: {pub_date.strftime('%Y-%m-%d')} -> {'í¬í•¨' if is_in_range else 'ì œì™¸'}")
            
            if is_in_range:
                filtered_news.append(news)
        
        logger.info(f"ğŸ“¤ í•„í„°ë§ í›„ ë‰´ìŠ¤ ìˆ˜: {len(filtered_news)}")
        
    # ê²°ê³¼ê°€ ì—†ì–´ë„ ë‹¹ì¼ë§Œ ìœ ì§€ (ì™„í™”í•˜ì§€ ì•ŠìŒ)
        
        return filtered_news
    
    def get_news_text(self, news_items: List[Dict]) -> str:
        """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
        if not news_items:
            return ""
        
        texts = []
        for news in news_items:
            # ì œëª©, ì„¤ëª…, ë‚´ìš©ì„ ëª¨ë‘ ê²°í•©
            text_parts = []
            
            if news.get('title'):
                text_parts.append(news['title'])
            
            if news.get('description'):
                text_parts.append(news['description'])
            
            if news.get('content'):
                # contentê°€ ìˆìœ¼ë©´ description ëŒ€ì‹  ì‚¬ìš©
                text_parts.append(news['content'])
            
            # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
            text = '. '.join(text_parts)
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = ' '.join(text.split())
            
            if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                texts.append(text)
        
        combined_text = ' | '.join(texts)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        if len(combined_text) > 8000:
            combined_text = combined_text[:8000] + "..."
        
        return combined_text

if __name__ == "__main__":
    collector = NewsCollector()
    test_date = datetime(2024, 7, 15)
    print("=== GNews API í…ŒìŠ¤íŠ¸ ===")
    news = collector.collect_company_news("Apple Inc.", "AAPL", test_date)
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
    if news:
        print("\nì²« ë²ˆì§¸ ë‰´ìŠ¤:")
        print(f"ì œëª©: {news[0]['title']}")
        print(f"ì„¤ëª…: {news[0]['description'][:200]}...")
        print(f"ì¶œì²˜: {news[0]['source']}")
        print(f"ë‚ ì§œ: {news[0]['published_date']}")
