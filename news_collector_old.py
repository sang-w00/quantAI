import requests
from datetime import datetime, timedelta
import time
from typing import List, Dict
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self, gnews_api_key: str = "44cc3fd97fb6417d55aeb8d962f7a831"):
        self.gnews_api_key = gnews_api_key
        self.gnews_base_url = "https://gnews.io/api/v4/search"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.last_api_call = 0
        self.min_call_interval = 1.0  # 1ì´ˆ ê°„ê²©
        
    def _wait_for_rate_limit(self):
        """API í˜¸ì¶œ ê°„ê²© ì œí•œ"""
        current_time = time.time()
        time_since_last_call = current_time - self.last_api_call
        
        if time_since_last_call < self.min_call_interval:
            wait_time = self.min_call_interval - time_since_last_call
            logger.info(f"â³ API í˜¸ì¶œ ì œí•œìœ¼ë¡œ {wait_time:.1f}ì´ˆ ëŒ€ê¸°")
            time.sleep(wait_time)
        
        self.last_api_call = time.time()
        
    def search_gnews(self, query: str, from_date: datetime, to_date: datetime, max_articles: int = 10) -> List[Dict]:
        """GNews APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰"""
        self._wait_for_rate_limit()
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD)
        from_str = from_date.strftime('%Y-%m-%d')
        to_str = to_date.strftime('%Y-%m-%d')
        
        # API íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            'q': query,
            'token': self.gnews_api_key,
            'lang': 'en',
            'country': 'us',
            'max': min(max_articles, 100),  # API ìµœëŒ€ 100ê°œ ì œí•œ
            'from': from_str,
            'to': to_str,
            'sortby': 'relevance'
        }
        
        try:
            logger.info(f"ğŸ” GNews API ê²€ìƒ‰: '{query}' ({from_str} ~ {to_str})")
            
            response = requests.get(self.gnews_base_url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                logger.info(f"ğŸ“° GNews API: {len(articles)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                
                # ë‰´ìŠ¤ ë°ì´í„° ë³€í™˜
                news_items = []
                for article in articles:
                    try:
                        # ë‚ ì§œ íŒŒì‹±
                        published_date = None
                        if 'publishedAt' in article:
                            published_date = datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00'))
                        
                        news_item = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'content': article.get('content', ''),
                            'link': article.get('url', ''),
                            'published_date': published_date,
                            'source': article.get('source', {}).get('name', 'Unknown'),
                            'image': article.get('image', '')
                        }
                        news_items.append(news_item)
                        
                    except Exception as e:
                        logger.warning(f"ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                return news_items
                
            elif response.status_code == 403:
                logger.error("âŒ GNews API ì¸ì¦ ì‹¤íŒ¨ - API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                return []
            elif response.status_code == 429:
                logger.warning("âš ï¸ GNews API í˜¸ì¶œ ì œí•œ ì´ˆê³¼ - ì ì‹œ ëŒ€ê¸°")
                time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
                return []
            else:
                logger.error(f"âŒ GNews API ì˜¤ë¥˜: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"GNews API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def get_company_keywords(self, company_name: str, ticker: str) -> List[str]:
        """íšŒì‚¬ë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        keywords = [ticker]
        
        # ê¸°ë³¸ íšŒì‚¬ëª…
        keywords.append(f'"{company_name}"')
        
        # íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬
        special_mappings = {
            'AAPL': ['Apple Inc', 'iPhone', 'iPad', 'Mac'],
            'MSFT': ['Microsoft', 'Windows', 'Office', 'Azure'],
            'GOOGL': ['Google', 'Alphabet', 'YouTube', 'Android'],
            'GOOG': ['Google', 'Alphabet', 'YouTube', 'Android'],
            'META': ['Meta', 'Facebook', 'Instagram', 'WhatsApp'],
            'TSLA': ['Tesla', 'Elon Musk', 'electric vehicle'],
            'AMZN': ['Amazon', 'AWS', 'Jeff Bezos'],
            'NVDA': ['NVIDIA', 'GPU', 'graphics card', 'AI chip'],
            'NFLX': ['Netflix', 'streaming'],
            'AVGO': ['Broadcom']
        }
        
        if ticker in special_mappings:
            keywords.extend(special_mappings[ticker])
        
        return keywords

if __name__ == "__main__":
    collector = NewsCollector()
    test_date = datetime(2024, 7, 15)
    print("=== GNews API í…ŒìŠ¤íŠ¸ ===")
    news = collector.collect_company_news("Apple Inc.", "AAPL", test_date)
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
    if news:
        print("\nì²« ë²ˆì§¸ ë‰´ìŠ¤:")
        print(f"ì œëª©: {news[0]['title']}")
        print(f"ì„¤ëª…: {news[0]['description'][:200]}...")
        print(f"ì¶œì²˜: {news[0]['source']}")
        print(f"ë‚ ì§œ: {news[0]['published_date']}")
    
    def filter_news_by_company(self, news_items: List[Dict], company_name: str, ticker: str) -> List[Dict]:
        """íšŒì‚¬ëª…ì´ë‚˜ í‹°ì»¤ê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (ê°œì„ ëœ ë§¤ì¹­)"""
        filtered_news = []
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¤€ë¹„ (ë” í¬ê´„ì )
        keywords = [
            ticker.upper(),
            ticker.lower(),
            company_name.lower(),
            # ì¼ë°˜ì ì¸ íšŒì‚¬ëª… ë³€í˜•ë“¤
            company_name.replace(' Inc.', '').replace(' Corporation', '').replace(' Corp.', '').replace(' Ltd.', '').lower(),
            company_name.split()[0].lower() if ' ' in company_name else company_name.lower()
        ]
        
        # íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬
        special_mappings = {
            'AAPL': ['apple', 'iphone', 'ipad', 'mac'],
            'MSFT': ['microsoft', 'windows', 'office', 'azure'],
            'GOOGL': ['google', 'alphabet', 'youtube', 'android'],
            'GOOG': ['google', 'alphabet', 'youtube', 'android'],
            'META': ['meta', 'facebook', 'instagram', 'whatsapp'],
            'TSLA': ['tesla', 'elon musk', 'electric vehicle', 'ev'],
            'AMZN': ['amazon', 'aws', 'bezos'],
            'NVDA': ['nvidia', 'gpu', 'ai chip', 'graphics'],
            'NFLX': ['netflix', 'streaming'],
            'AVGO': ['broadcom']
        }
        
        if ticker in special_mappings:
            keywords.extend(special_mappings[ticker])
        
        for news in news_items:
            title_lower = news['title'].lower()
            desc_lower = news['description'].lower()
            combined_text = f"{title_lower} {desc_lower}"
            
            # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì„ íƒ
            if any(keyword in combined_text for keyword in keywords):
                filtered_news.append(news)
        
        return filtered_news
    
    def filter_news_by_date(self, news_items: List[Dict], target_date: datetime) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        filtered_news = []
        for news in news_items:
            if news['published_date'] and start_date <= news['published_date'] <= end_date:
                filtered_news.append(news)
        
        return filtered_news
    
    def collect_company_news(self, company_name: str, ticker: str, target_date: datetime) -> List[Dict]:
        """íŠ¹ì • íšŒì‚¬ì˜ íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘ (GNews API ì‚¬ìš©)"""
        all_news = []
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ì „í›„ 1ì¼ì”© í¬í•¨í•˜ì—¬ 3ì¼ ë²”ìœ„)
        start_date = target_date - timedelta(days=1)
        end_date = target_date + timedelta(days=1)
        
        logger.info(f"ğŸ“° {ticker} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"ğŸ“… ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
        # íšŒì‚¬ë³„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keywords = self.get_company_keywords(company_name, ticker)
        
        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for i, keyword in enumerate(keywords[:3], 1):  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            try:
                logger.info(f"ğŸ” í‚¤ì›Œë“œ {i}: '{keyword}' ê²€ìƒ‰ ì¤‘...")
                
                # GNews APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
                news_items = self.search_gnews(keyword, start_date, end_date, max_articles=20)
                
                if news_items:
                    # ëŒ€ìƒ ë‚ ì§œ í•„í„°ë§
                    filtered_news = self.filter_news_by_date(news_items, target_date)
                    all_news.extend(filtered_news)
                    
                    logger.info(f"  âœ… '{keyword}': {len(filtered_news)}ê°œ ê´€ë ¨ ë‰´ìŠ¤")
                else:
                    logger.info(f"  âŒ '{keyword}': ë‰´ìŠ¤ ì—†ìŒ")
                
                # API í˜¸ì¶œ ì œí•œ ëŒ€ê¸°
                time.sleep(1)
                
            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (ì œëª©ê³¼ URL ê¸°ì¤€)
        seen_items = set()
        unique_news = []
        
        for news in all_news:
            # ì œëª©ê³¼ URLì„ ê²°í•©í•œ í‚¤ ìƒì„±
            key = f"{news['title'].strip().lower()}_{news['link']}"
            
            if key not in seen_items and len(news['title'].strip()) > 10:
                seen_items.add(key)
                unique_news.append(news)
        
        logger.info(f"ğŸ¯ {ticker} ({target_date.strftime('%Y-%m-%d')}): ì´ {len(unique_news)}ê°œ ìœ ë‹ˆí¬ ë‰´ìŠ¤ ìˆ˜ì§‘")
        
        # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìš”ì•½ ë¡œê¹…
        if unique_news:
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ì¶œì²˜ ë¶„í¬:")
            sources = {}
            for news in unique_news:
                source = news['source']
                sources[source] = sources.get(source, 0) + 1
            
            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  - {source}: {count}ê°œ")
        
        return unique_news
    
    def filter_news_by_date(self, news_items: List[Dict], target_date: datetime) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (3ì¼ ë²”ìœ„)"""
        # ë‚ ì§œ ë²”ìœ„ë¥¼ 3ì¼ë¡œ í™•ëŒ€ (ì „ë‚ , ë‹¹ì¼, ë‹¤ìŒë‚ )
        start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)
        end_date = target_date.replace(hour=23, minute=59, second=59, microsecond=999999) + timedelta(days=1)
        
        filtered_news = []
        for news in news_items:
            # ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš° í¬í•¨ (ìµœì‹  ë‰´ìŠ¤ë¡œ ê°„ì£¼)
            if not news['published_date']:
                filtered_news.append(news)
                continue
            
            # UTC ì‹œê°„ì„ ë¡œì»¬ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            pub_date = news['published_date']
            if pub_date.tzinfo:
                pub_date = pub_date.replace(tzinfo=None)
            
            if start_date <= pub_date <= end_date:
                filtered_news.append(news)
        
        return filtered_news
    
    def get_news_text(self, news_items: List[Dict]) -> str:
        """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
        if not news_items:
            return ""
        
        texts = []
        for news in news_items:
            # ì œëª©, ì„¤ëª…, ë‚´ìš©ì„ ëª¨ë‘ ê²°í•©
            text_parts = []
            
            if news.get('title'):
                text_parts.append(news['title'])
            
            if news.get('description'):
                text_parts.append(news['description'])
            
            if news.get('content'):
                # contentê°€ ìˆìœ¼ë©´ description ëŒ€ì‹  ì‚¬ìš©
                text_parts.append(news['content'])
            
            # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
            text = '. '.join(text_parts)
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = ' '.join(text.split())
            
            if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                texts.append(text)
        
        combined_text = ' | '.join(texts)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
        if len(combined_text) > 8000:
            combined_text = combined_text[:8000] + "..."
        
        return combined_text
    
    def get_news_text(self, news_items: List[Dict]) -> str:
        """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
        if not news_items:
            return ""
        
        texts = []
        for news in news_items:
            # ì œëª©ê³¼ ì„¤ëª…ì„ ê²°í•©
            text = f"{news['title']}. {news['description']}"
            # HTML íƒœê·¸ ì œê±°
            text = BeautifulSoup(text, 'html.parser').get_text()
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            text = ' '.join(text.split())
            texts.append(text)
        
        return ' '.join(texts)

if __name__ == "__main__":
    collector = NewsCollector()
    test_date = datetime(2024, 1, 15)
    news = collector.collect_company_news("Apple Inc.", "AAPL", test_date)
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
    if news:
        print("ì²« ë²ˆì§¸ ë‰´ìŠ¤:")
        print(f"ì œëª©: {news[0]['title']}")
        print(f"ì„¤ëª…: {news[0]['description'][:200]}...")
