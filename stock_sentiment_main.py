import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import logging
from typing import Dict, List
from tqdm import tqdm
import time
import subprocess
import holidays

from nasdaq100_companies import get_nasdaq100_companies
from news_collector import NewsCollector
from sentiment_analyzer import SentimentAnalyzer

def get_windows_host_ip():
    """WSLì—ì„œ Windows í˜¸ìŠ¤íŠ¸ IP ì£¼ì†Œë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” í•¨ìˆ˜"""
    try:
        result = subprocess.run(['ip', 'route', 'show'], capture_output=True, text=True)
        for line in result.stdout.split('\n'):
            if 'default' in line:
                parts = line.split()
                if len(parts) >= 3:
                    return parts[2]
    except Exception as e:
        logging.warning(f"Windows í˜¸ìŠ¤íŠ¸ IP ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ê°’ ë°˜í™˜
    return "172.19.144.1"

def get_windows_host_ip():
    """WSLì—ì„œ Windows í˜¸ìŠ¤íŠ¸ IP ì£¼ì†Œë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” í•¨ìˆ˜"""
    try:
        result = subprocess.run(['ip', 'route', 'show'], capture_output=True, text=True)
        for line in result.stdout.split('\n'):
            if 'default' in line:
                parts = line.split()
                if len(parts) >= 3:
                    return parts[2]
    except Exception as e:
        logging.warning(f"Windows í˜¸ìŠ¤íŠ¸ IP ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ê°’ ë°˜í™˜
    return "172.19.144.1"

def setup_result_directory(start_date: str, end_date: str) -> str:
    """ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    # ê¸°ê°„ì„ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš© (YYYY-MM-DD_to_YYYY-MM-DD)
    period_folder = f"{start_date}_to_{end_date}"
    result_dir = os.path.join("results", period_folder)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(result_dir, exist_ok=True)
    
    return result_dir

def setup_logging(result_dir: str):
    """ë¡œê¹… ì„¤ì • (ê²°ê³¼ ë””ë ‰í† ë¦¬ì— ë¡œê·¸ íŒŒì¼ ì €ì¥)"""
    log_file = os.path.join(result_dir, "sentiment_analysis.log")
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    logging.getLogger().handlers.clear()
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

# ì´ˆê¸° ë¡œê¹… ì„¤ì • (ì„ì‹œ)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockSentimentAnalyzer:
    def __init__(self, ollama_host: str = "http://localhost:11434", model: str = "gpt-oss:20b", result_dir: str = "."):
        self.news_collector = NewsCollector(polygon_api_key="q96aIisakzHv_c7jRaoginkjRj8zGWu3")
        self.sentiment_analyzer = SentimentAnalyzer(ollama_host, model)
        self.nasdaq100_symbols, self.company_names = get_nasdaq100_companies()
        self.result_dir = result_dir
        
        # ë¯¸êµ­ ê³µíœ´ì¼ ì„¤ì •
        self.us_holidays = holidays.US()
        
    def is_business_day(self, date: datetime) -> bool:
        """ì˜ì—…ì¼ì¸ì§€ í™•ì¸ (ì£¼ë§ ë° ê³µíœ´ì¼ ì œì™¸)"""
        # ì£¼ë§ í™•ì¸ (í† ìš”ì¼=5, ì¼ìš”ì¼=6)
        if date.weekday() >= 5:
            return False
        
        # ê³µíœ´ì¼ í™•ì¸
        if date.date() in self.us_holidays:
            return False
            
        return True
        
    def generate_date_range(self, start_date: str, end_date: str) -> List[datetime]:
        """ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì˜ì—…ì¼ë§Œ, ì£¼ë§ ë° ê³µíœ´ì¼ ì œì™¸)"""
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        
        dates = []
        current = start
        while current <= end:
            # ì˜ì—…ì¼ë§Œ ì¶”ê°€
            if self.is_business_day(current):
                dates.append(current)
            current += timedelta(days=1)
        
        return dates
    
    def analyze_single_day_company(self, symbol: str, company_name: str, target_date: datetime) -> float:
        """íŠ¹ì • ë‚ ì§œ, íŠ¹ì • íšŒì‚¬ì˜ ê°ì„± ë¶„ì„"""
        try:
            # ë‰´ìŠ¤ ìˆ˜ì§‘
            news_items = self.news_collector.collect_company_news(company_name, symbol, target_date)
            
            # ë‰´ìŠ¤ ìƒì„¸ ë¡œê¹…
            logger.info(f"=== {symbol} ({target_date.strftime('%Y-%m-%d')}) ë‰´ìŠ¤ ë¶„ì„ ===")
            
            if not news_items:
                logger.info(f"{symbol}: ë‰´ìŠ¤ ì—†ìŒ, ì¤‘ë¦½ê°’(0) ë°˜í™˜")
                logger.info(f"{'='*50}")
                return 0.0
            
            # ê° ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„¸ ë¡œê¹…
            logger.info(f"ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ {len(news_items)}ê°œ:")
            for i, news in enumerate(news_items, 1):
                logger.info(f"  [{i}] ì œëª©: {news['title']}")
                logger.info(f"      ì¶œì²˜: {news['source']}")
                logger.info(f"      ë‚ ì§œ: {news['published_date']}")
                logger.info(f"      ì„¤ëª…: {news['description'][:200]}{'...' if len(news['description']) > 200 else ''}")
                logger.info(f"      ë§í¬: {news['link']}")
                logger.info(f"      ---")
            
            # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ê²°í•©
            news_text = self.news_collector.get_news_text(news_items)
            
            if not news_text.strip():
                logger.info(f"{symbol}: ë¹ˆ í…ìŠ¤íŠ¸, ì¤‘ë¦½ê°’(0) ë°˜í™˜")
                logger.info(f"{'='*50}")
                return 0.0
            
            # ê²°í•©ëœ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë¡œê¹…
            logger.info(f"ğŸ“ ê²°í•©ëœ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ (ì´ {len(news_text)} ë¬¸ì):")
            logger.info(f"í…ìŠ¤íŠ¸ ì‹œì‘ 500ì: {news_text[:500]}...")
            if len(news_text) > 1000:
                logger.info(f"í…ìŠ¤íŠ¸ ë 500ì: ...{news_text[-500:]}")
            logger.info(f"---")
            
            # ê°ì„± ë¶„ì„ ì‹œì‘ ë¡œê¹…
            logger.info(f"ğŸ¤– ê°ì„±ë¶„ì„ ì‹œì‘ (ëª¨ë¸: gpt-oss:20b)")
            
            # ê°ì„± ë¶„ì„
            sentiment_score = self.sentiment_analyzer.analyze_sentiment(news_text)
            
            # ê°ì„± ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            logger.info(f"ğŸ¯ {symbol} ê°ì„±ë¶„ì„ ì™„ë£Œ:")
            logger.info(f"   ğŸ“Š ê°ì„± ì ìˆ˜: {sentiment_score:.4f}")
            logger.info(f"   ğŸ“ˆ ë¶„ì„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(news_text):,} ë¬¸ì")
            logger.info(f"   ğŸ“° ë‰´ìŠ¤ ê°œìˆ˜: {len(news_items)}ê°œ")
            
            # ê°ì„± ì ìˆ˜ ìƒì„¸ í•´ì„
            if sentiment_score > 0.5:
                interpretation = "ë§¤ìš° ê¸ì •ì  ğŸš€"
                market_impact = "ì£¼ê°€ ìƒìŠ¹ ìš”ì¸"
            elif sentiment_score > 0.2:
                interpretation = "ê¸ì •ì  ğŸ“ˆ"
                market_impact = "ì•½ê°„ì˜ ì£¼ê°€ ìƒìŠ¹ ìš”ì¸"
            elif sentiment_score > -0.2:
                interpretation = "ì¤‘ë¦½ì  â¡ï¸"
                market_impact = "ì£¼ê°€ì— ì¤‘ë¦½ì  ì˜í–¥"
            elif sentiment_score > -0.5:
                interpretation = "ë¶€ì •ì  ğŸ“‰"
                market_impact = "ì•½ê°„ì˜ ì£¼ê°€ í•˜ë½ ìš”ì¸"
            else:
                interpretation = "ë§¤ìš° ë¶€ì •ì  ğŸ’¥"
                market_impact = "ì£¼ê°€ í•˜ë½ ìš”ì¸"
            
            logger.info(f"   ğŸ’¡ í•´ì„: {interpretation}")
            logger.info(f"   ğŸ“Š ì‹œì¥ ì˜í–¥: {market_impact}")
            
            # ì ìˆ˜ ë²”ì£¼ë³„ ë¶„ë¥˜
            if sentiment_score > 0:
                logger.info(f"   âœ… ë¶„ë¥˜: ê¸ì •ì  ë‰´ìŠ¤ (+{sentiment_score:.4f})")
            elif sentiment_score < 0:
                logger.info(f"   âŒ ë¶„ë¥˜: ë¶€ì •ì  ë‰´ìŠ¤ ({sentiment_score:.4f})")
            else:
                logger.info(f"   âšª ë¶„ë¥˜: ì¤‘ë¦½ì  ë‰´ìŠ¤ (0.0000)")
            
            # ì‹ ë¢°ë„ í‰ê°€ (ë‰´ìŠ¤ ê°œìˆ˜ ê¸°ë°˜)
            if len(news_items) >= 5:
                confidence = "ë†’ìŒ"
            elif len(news_items) >= 3:
                confidence = "ë³´í†µ"
            else:
                confidence = "ë‚®ìŒ"
            
            logger.info(f"   ğŸ¯ ì‹ ë¢°ë„: {confidence} (ë‰´ìŠ¤ {len(news_items)}ê°œ ê¸°ë°˜)")
            logger.info(f"{'='*70}")
            
            return sentiment_score
            
        except Exception as e:
            error_msg = f"{symbol} ({target_date.strftime('%Y-%m-%d')}) ë¶„ì„ ì˜¤ë¥˜: {e}"
            logger.error(error_msg)
            logger.error(f"{'='*50}")
            return 0.0
    
    def analyze_period(self, start_date: str, end_date: str, output_filename: str = "nasdaq100_sentiment.csv") -> pd.DataFrame:
        """ê¸°ê°„ ë™ì•ˆì˜ ëª¨ë“  ê¸°ì—… ê°ì„± ë¶„ì„"""
        dates = self.generate_date_range(start_date, end_date)
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
        output_file = os.path.join(self.result_dir, output_filename)
        temp_file = os.path.join(self.result_dir, f"{output_filename}.temp")
        
        logger.info(f"ë¶„ì„ ê¸°ê°„: {start_date} ~ {end_date} ({len(dates)}ì¼)")
        logger.info(f"ë¶„ì„ ëŒ€ìƒ: ë‚˜ìŠ¤ë‹¥ 100 ê¸°ì—… {len(self.nasdaq100_symbols)}ê°œ")
        logger.info(f"ì´ ë¶„ì„ ì‘ì—…: {len(dates) * len(self.nasdaq100_symbols)}ê°œ")
        logger.info(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_file}")
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
        df = pd.DataFrame(index=[d.strftime('%Y-%m-%d') for d in dates], 
                         columns=self.nasdaq100_symbols)
        df = df.fillna(0.0)  # ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì„¤ì •
        
        # ì§„í–‰ìƒí™© ì¶”ì 
        total_tasks = len(dates) * len(self.nasdaq100_symbols)
        completed_tasks = 0
        
        with tqdm(total=total_tasks, desc="ê°ì„± ë¶„ì„ ì§„í–‰") as pbar:
            for date in dates:
                date_str = date.strftime('%Y-%m-%d')
                logger.info(f"ë‚ ì§œ {date_str} ë¶„ì„ ì‹œì‘")
                
                for symbol in self.nasdaq100_symbols:
                    company_name = self.company_names[symbol]
                    
                    try:
                        sentiment_score = self.analyze_single_day_company(symbol, company_name, date)
                        df.loc[date_str, symbol] = sentiment_score
                        
                        # ì¤‘ê°„ ì €ì¥ (10ê°œ ì‘ì—…ë§ˆë‹¤)
                        if completed_tasks % 10 == 0:
                            df.to_csv(temp_file)
                        
                    except Exception as e:
                        logger.error(f"{symbol} ({date_str}) ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        df.loc[date_str, symbol] = 0.0
                    
                    completed_tasks += 1
                    pbar.update(1)
                    
                    # Rate limiting (ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€)
                    time.sleep(1)
                
                # í•˜ë£¨ ì™„ë£Œ í›„ ì €ì¥
                df.to_csv(temp_file)
                logger.info(f"ë‚ ì§œ {date_str} ë¶„ì„ ì™„ë£Œ")
        
        # ìµœì¢… ì €ì¥
        df.to_csv(output_file)
        logger.info(f"ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_file):
            os.remove(temp_file)
        
        return df
    
    def load_and_resume(self, output_filename: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ê³  ë¯¸ì™„ë£Œ ë¶€ë¶„ì„ ì´ì–´ì„œ ë¶„ì„"""
        output_file = os.path.join(self.result_dir, output_filename)
        temp_file = os.path.join(self.result_dir, f"{output_filename}.temp")
        
        if os.path.exists(temp_file):
            logger.info(f"ì„ì‹œ íŒŒì¼ì—ì„œ ë¶„ì„ ì¬ê°œ: {temp_file}")
            df = pd.read_csv(temp_file, index_col=0)
        elif os.path.exists(output_file):
            logger.info(f"ê¸°ì¡´ íŒŒì¼ì—ì„œ ë¶„ì„ ì¬ê°œ: {output_file}")
            df = pd.read_csv(output_file, index_col=0)
        else:
            logger.info("ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘")
            return self.analyze_period(start_date, end_date, output_filename)
        
        # ë¯¸ì™„ë£Œ ë¶€ë¶„ ì°¾ê¸°
        dates = self.generate_date_range(start_date, end_date)
        
        for date in dates:
            date_str = date.strftime('%Y-%m-%d')
            
            if date_str not in df.index:
                logger.info(f"ëˆ„ë½ëœ ë‚ ì§œ {date_str}ë¶€í„° ë¶„ì„ ì¬ê°œ")
                # ë‚˜ë¨¸ì§€ ê¸°ê°„ ë¶„ì„
                remaining_start = date_str
                remaining_df = self.analyze_period(remaining_start, end_date, output_filename)
                return remaining_df
        
        logger.info("ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return df
    
    def get_summary_statistics(self, df: pd.DataFrame) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ í†µê³„"""
        summary = {
            'total_days': len(df),
            'total_companies': len(df.columns),
            'total_analysis_points': df.size,
            'zero_values_count': (df == 0).sum().sum(),
            'positive_sentiment_ratio': (df > 0).sum().sum() / df.size,
            'negative_sentiment_ratio': (df < 0).sum().sum() / df.size,
            'neutral_sentiment_ratio': (df == 0).sum().sum() / df.size,
            'mean_sentiment_by_company': df.mean().to_dict(),
            'mean_sentiment_by_date': df.mean(axis=1).to_dict()
        }
        
        return summary

def main():
    # í…ŒìŠ¤íŠ¸ ì„¤ì • (ìƒìœ„ 10ê°œ ê¸°ì—…)
    START_DATE = "2025-08-01"  # ì‹œì‘ ë‚ ì§œ
    END_DATE = "2025-08-21"    # ì¢…ë£Œ ë‚ ì§œ 
    OUTPUT_FILENAME = "nasdaq10_sentiment_analysis.csv"
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì„¤ì •
    result_dir = setup_result_directory(START_DATE, END_DATE)
    
    # ë¡œê¹… ì„¤ì • (ê²°ê³¼ ë””ë ‰í† ë¦¬ì— ë¡œê·¸ ì €ì¥)
    logger = setup_logging(result_dir)
    
    # WSLì—ì„œ Windows í˜¸ìŠ¤íŠ¸ì˜ Ollama ì„œë²„ ì ‘ê·¼
    windows_ip = get_windows_host_ip()
    OLLAMA_HOST = f"http://{windows_ip}:11434"
    
    # Ollama ì„¤ì • í™•ì¸
    print("=== ë‚˜ìŠ¤ë‹¥ 10 ì£¼ì‹ ê°ì„±ë¶„ì„ í…ŒìŠ¤íŠ¸ ===")
    print("Windowsì—ì„œ ì‹¤í–‰ ì¤‘ì¸ Ollama ì„œë²„ì— ì—°ê²°í•©ë‹ˆë‹¤.")
    print(f"ê°ì§€ëœ Windows í˜¸ìŠ¤íŠ¸ IP: {windows_ip}")
    print(f"Ollama í˜¸ìŠ¤íŠ¸: {OLLAMA_HOST}")
    print(f"í…ŒìŠ¤íŠ¸ ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    print("í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: ë‚˜ìŠ¤ë‹¥ ìƒìœ„ 10ê°œ ê¸°ì—…")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {result_dir}/")
    print("Windowsì—ì„œ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull gpt-oss:20b")
    input("ì¤€ë¹„ê°€ ë˜ë©´ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = StockSentimentAnalyzer(ollama_host=OLLAMA_HOST, result_dir=result_dir)
    
    # ìƒìœ„ 10ê°œ ê¸°ì—…ë§Œ ì„ íƒ (ì‹œê°€ì´ì•¡ ê¸°ì¤€)
    top_10_symbols = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'GOOGL', 'GOOG', 'META', 'TSLA', 'AVGO', 'COST']
    analyzer.nasdaq100_symbols = top_10_symbols
    
    print(f"ì„ íƒëœ ê¸°ì—…: {', '.join(top_10_symbols)}")
    
    # ë¶„ì„ ì •ë³´ ë¡œê¹…
    logger.info("="*50)
    logger.info("ë‚˜ìŠ¤ë‹¥ 10 ì£¼ì‹ ê°ì„±ë¶„ì„ ì‹œì‘")
    logger.info(f"ë¶„ì„ ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info(f"ë¶„ì„ ëŒ€ìƒ: {', '.join(top_10_symbols)}")
    logger.info(f"ê²°ê³¼ ë””ë ‰í† ë¦¬: {result_dir}")
    logger.info("="*50)
    
    # ë¶„ì„ ì‹¤í–‰ (ì¬ê°œ ê°€ëŠ¥)
    try:
        df = analyzer.load_and_resume(OUTPUT_FILENAME, START_DATE, END_DATE)
        
        # ê²°ê³¼ ìš”ì•½
        summary = analyzer.get_summary_statistics(df)
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥ ë° ë¡œê¹…
        summary_text = f"""
=== ë¶„ì„ ê²°ê³¼ ìš”ì•½ ===
ë¶„ì„ ê¸°ê°„: {START_DATE} ~ {END_DATE}
ì´ ë¶„ì„ì¼ìˆ˜: {summary['total_days']}ì¼
ì´ ê¸°ì—…ìˆ˜: {summary['total_companies']}ê°œ
ì´ ë¶„ì„ í¬ì¸íŠ¸: {summary['total_analysis_points']}ê°œ
ë‰´ìŠ¤ ì—†ìŒ(0ê°’): {summary['zero_values_count']}ê°œ ({summary['neutral_sentiment_ratio']*100:.1f}%)
ê¸ì • ê°ì„±: {summary['positive_sentiment_ratio']*100:.1f}%
ë¶€ì • ê°ì„±: {summary['negative_sentiment_ratio']*100:.1f}%

ê²°ê³¼ íŒŒì¼: {os.path.join(result_dir, OUTPUT_FILENAME)}
ë¡œê·¸ íŒŒì¼: {os.path.join(result_dir, 'sentiment_analysis.log')}
        """
        
        print(summary_text)
        logger.info(summary_text)
        
        # ìš”ì•½ í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        summary_file = os.path.join(result_dir, "analysis_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ìš”ì•½ í†µê³„ ì €ì¥: {summary_file}")
        print("ë¶„ì„ ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ë¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ë¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ì‹œ íŒŒì¼ì´ ì €ì¥ë˜ì–´ ë‚˜ì¤‘ì— ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
